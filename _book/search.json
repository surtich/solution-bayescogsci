[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bayescogsci exercises",
    "section": "",
    "text": "Preface\nExercise solutions of An Introduction to Bayesian Data Analysis for Cognitive Science Book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.1-practice-using-the-pnorm-functionpart-1",
    "href": "chapter01.html#exercise-1.1-practice-using-the-pnorm-functionpart-1",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.1 Practice using the pnorm() function–Part 1",
    "text": "Exercise 1.1 Practice using the pnorm() function–Part 1\nGiven a normal distribution with mean 500 and standard deviation 100, use the pnorm function to calculate the probability of obtaining values between 200 and 800 from this distribution.\n\npnorm(800, mean = 500, sd = 100) - pnorm(200, mean = 500, sd = 100) \n\n[1] 0.9973002\n\n1-2*pnorm(200, mean = 500, sd = 100)\n\n[1] 0.9973002",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.2-practice-using-the-pnorm-functionpart-2",
    "href": "chapter01.html#exercise-1.2-practice-using-the-pnorm-functionpart-2",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.2 Practice using the pnorm() function–Part 2",
    "text": "Exercise 1.2 Practice using the pnorm() function–Part 2\nCalculate the following probabilities. Given a normal distribution with mean 800 and standard deviation 150, what is the probability of obtaining:\na score of 700 or less a score of 900 or more a score of 800 or more\n\nc(pnorm(700, 800, 150), pnorm(900, 800, 150, lower.tail = FALSE), 1-pnorm(800, 800, 150))\n\n[1] 0.2524925 0.2524925 0.5000000",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.3-practice-using-the-pnorm-functionpart-3",
    "href": "chapter01.html#exercise-1.3-practice-using-the-pnorm-functionpart-3",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.3 Practice using the pnorm() function–Part 3",
    "text": "Exercise 1.3 Practice using the pnorm() function–Part 3\nGiven a normal distribution with mean 600 and standard deviation 200, what is the probability of obtaining:\na score of 550 or less. a score between 300 and 800. a score of 900 or more.\n\nc(pnorm(550, 600, 150), pnorm(800, 600, 150)-pnorm(300, 600, 150), 1-pnorm(900, 600, 150))\n\n[1] 0.36944134 0.88603865 0.02275013",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.4-practice-using-the-qnorm-functionpart1",
    "href": "chapter01.html#exercise-1.4-practice-using-the-qnorm-functionpart1",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.4 Practice using the qnorm() function–Part1",
    "text": "Exercise 1.4 Practice using the qnorm() function–Part1\nConsider a normal distribution with mean 1 and standard deviation 1. Compute the lower and upper boundaries such that:\nthe area (the probability) to the left of the lower boundary is 0.10. the area (the probability) to the right of the upper boundary is 0.90.\n\nc(qnorm(.1, 1, 1), qnorm(.9, 1, 1, lower.tail = FALSE))\n\n[1] -0.2815516 -0.2815516",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.5-practice-using-the-qnorm-functionpart2",
    "href": "chapter01.html#exercise-1.5-practice-using-the-qnorm-functionpart2",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.5 Practice using the qnorm() function–Part2",
    "text": "Exercise 1.5 Practice using the qnorm() function–Part2\nGiven a normal distribution with mean 650 and standard deviation 125. There exist two quantiles, the lower quantile q1 and the upper quantile q2, that are equidistant from the mean 650, such that the area under the curve of the normal between q1 and q2 is 80%. Find q1 and q2.\n\nx &lt;- c(qnorm(.1, 650, 125), qnorm(.1, 650, 125, lower.tail = FALSE)) \npnorm(x[2], 650, 125) - pnorm(x[1], 650, 125) \n\n[1] 0.8",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.6-practice-getting-summaries-from-samplespart-1",
    "href": "chapter01.html#exercise-1.6-practice-getting-summaries-from-samplespart-1",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.6 Practice getting summaries from samples–Part 1",
    "text": "Exercise 1.6 Practice getting summaries from samples–Part 1\nGiven data that is generated as follows:\n\ndata_gen1 &lt;- rnorm(1000, 300, 200)\n\nCalculate the mean, variance, and the lower quantile q1 and the upper quantile q2, that are equidistant and such that the range of probability between them is 80%.\n\nc(mean=mean(data_gen1), var=var(data_gen1), quantile(data_gen1, c(.1, .9)))\n\n       mean         var         10%         90% \n  296.00915 39355.12453    39.01605   552.92524",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.7-practice-getting-summaries-from-samplespart-2.",
    "href": "chapter01.html#exercise-1.7-practice-getting-summaries-from-samplespart-2.",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.7 Practice getting summaries from samples–Part 2.",
    "text": "Exercise 1.7 Practice getting summaries from samples–Part 2.\nThis time we generate the data with a truncated normal distribution from the package extraDistr. The details of this distribution will be discussed later in section 4.1 and in the Box 4.1, but for now we can treat it as an unknown generative process:\n\nlibrary(extraDistr)\ndata_gen1 &lt;- rtnorm(1000, 300, 200, a = 0)\n\nUsing the sample data, calculate the mean, variance, and the lower quantile q1 and the upper quantile q2, such that the probability of observing values between these two quantiles is 80%.\n\nc(mean=mean(data_gen1), var=var(data_gen1), quantile(data_gen1, c(.1, .9)))\n\n      mean        var        10%        90% \n  327.6990 28366.5446   104.6224   551.4198",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.8-practice-with-a-variance-covariance-matrix-for-a-bivariate-distribution.",
    "href": "chapter01.html#exercise-1.8-practice-with-a-variance-covariance-matrix-for-a-bivariate-distribution.",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.8 Practice with a variance-covariance matrix for a bivariate distribution.",
    "text": "Exercise 1.8 Practice with a variance-covariance matrix for a bivariate distribution.\nSuppose that you have a bivariate distribution where one of the two random variables comes from a normal distribution with mean \\(μ_X=600\\) and standard deviation \\(σ_X=100\\), and the other from a normal distribution with mean \\(μ_Y=400\\) and standard deviation \\(σ_Y=50\\). The correlation \\(ρ_{XY}\\) between the two random variables is 0.4. Write down the variance-covariance matrix of this bivariate distribution as a matrix (with numerical values, not mathematical symbols), and then use it to generate 100 pairs of simulated data points. Plot the simulated data such that the relationship between the random variables \\(X\\) and \\(Y\\) is clear. Generate two sets of new data (\\(100\\) pairs of data points each) with correlation \\(-0.4\\) and \\(0\\), and plot these alongside the plot for the data with correlation \\(0.4\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.1-deriving-bayes-rule",
    "href": "chapter02.html#exercise-2.1-deriving-bayes-rule",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.1 Deriving Bayes’ rule",
    "text": "Exercise 2.1 Deriving Bayes’ rule\nLet \\(A\\) and \\(B\\) be two observable events. \\(P(A)\\) is the probability that \\(A\\) occurs, and \\(P(B)\\) is the probability that\n\\(B\\) occurs. \\(P(A|B)\\) is the conditional probability that \\(A\\) occurs given that \\(B\\) has happened. \\(P(A,B)\\) is the joint probability of \\(A\\) and \\(B\\) both occurring.\nYou are given the definition of conditional probability:\n\\[\n\\begin{equation}\nP(A|B)= \\frac{P(A,B)}{P(B)} \\hbox{ where } P(B)&gt;0\n\\end{equation}\n\\]\nUsing the above definition, and using the fact that \\(P(A,B)=P(B,A)\\) (i.e., the probability of\n\\(A\\) and \\(B\\) both occurring is the same as the probability of \\(B\\) and \\(A\\) both occurring), derive an expression for\n\\(P(B|A)\\). Show the steps clearly in the derivation.\n\n\\[\n\\begin{aligned}\nP(B|A) &= \\frac{P(B,A)}{P(A)} \\hbox{ where } P(A)&gt;0 \\\\\n       &= \\frac{P(A|B)P(B)}{P(A)}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.2-conjugate-forms-1",
    "href": "chapter02.html#exercise-2.2-conjugate-forms-1",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.2 Conjugate forms 1",
    "text": "Exercise 2.2 Conjugate forms 1\n\nComputing the general form of a PDF for a posterior\nSuppose you are given data \\(k\\) consisting of the number of successes, coming from a \\(\\text{Binomial}(n,θ)\\) distribution. Given \\(k\\) successes in \\(n\\) trials coming from a binomial distribution, we define a \\(\\text{Beta}(a,b)\\) prior on the parameter \\(θ\\). Write down the Beta distribution that represents the posterior, in terms of \\(a\\), \\(b\\), \\(n\\), and \\(k\\).\n\n\\[\n\\begin{aligned}\np(\\theta | n,k) &\\propto p(k |n,\\theta) p(\\theta) \\\\\n\\text{Beta}(a+k, b+n-k) &= \\text{Binomial}(k|n,θ)\\times\\text{Beta}(\\theta|a,b)\n\\end{aligned}\n\\]\n\n\n\nPractical application\nWe ask 10 yes/no questions from a subject, and the subject returns 0 correct answers. We assume a binomial likelihood function for these data. Also assume a \\(Beta(1,1)\\) prior on the parameter \\(θ\\), which represents the probability of success. Use the result you derived above to write down the posterior distribution of the \\(θ\\) parameter.\n\n\\[\n\\text{Beta}(1+0, 1+10-0) = \\text{Beta}(1, 11)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.3-conjugate-forms-2",
    "href": "chapter02.html#exercise-2.3-conjugate-forms-2",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.3 Conjugate forms 2",
    "text": "Exercise 2.3 Conjugate forms 2\nSuppose that we perform \\(n\\) independent trials until we get a success (e.g., a heads in a coin toss). For coin tosses, the possible outcomes could be, H, T. The probability of success in each trial is \\(θ\\). Then, the Geometric random variable, call it \\(X\\), gives us the probability of getting a success in\n\\(n\\) trials as follows:\n\\[\n\\begin{equation}\nProb(X=n)=\\theta(1-\\theta)^{ n-1}\\text{, where } n =1,2,...\n\\end{equation}\n\\] Let the prior on \\(θ\\) be \\(Beta(a,b)\\), a beta distribution with parameters \\(a\\),\\(b\\). The posterior distribution is a beta distribution with parameters \\(a^*\\) and \\(b^*\\). Determine these parameters in terms of \\(a\\), \\(b\\), and \\(n\\).\n\n\\[\n\\begin{aligned}\np(\\theta | n) &\\propto p(n |\\theta) p(\\theta) \\\\\np(\\theta | n) &\\propto \\theta(1-\\theta)^{ n-1}\\theta^{a-1}(1-\\theta)^{b-1} \\propto \\theta^a(1-\\theta)^{b+n-2} \\\\\np(\\theta | n) &= Beta(a^*,b^*) = Beta(a+1,b+n-1)\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.4-conjugate-forms-3",
    "href": "chapter02.html#exercise-2.4-conjugate-forms-3",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.4 Conjugate forms 3",
    "text": "Exercise 2.4 Conjugate forms 3\nThe Gamma distribution is defined in terms of the parameters a, b: Ga(a,b). If there is a random variable \\(Y\\) (where \\(y \\gt 0\\)) that has a Gamma distribution as a PDF (\\(Y\\sim \\text{Gamma}(a,b)\\)), then:\n\\[\n\\begin{equation}\nGa(y | a,b)=\\frac{b^a y^{a-1} \\exp\\{-by\\}}{\\Gamma(a)}\n\\end{equation}\n\\]\nSuppose that we have \\(n\\) data points, \\(x_1,…,x_n\\), that are drawn from an exponentially distributed. The exponential likelihood function is:\n\\[\n\\begin{equation}\np(x_1,\\dots,x_n | \\lambda)=\\lambda^n \\exp \\{-\\lambda \\sum_{i=1}^n x_i \\}\n\\end{equation}\n\\]\nIt turns out that if we assume a Ga(a,b) prior distribution for \\(λ\\) and the above Exponential likelihood, the posterior distribution of \\(λ\\) is a Gamma distribution. In other words, the Gamma(a,b) prior on the \\(λ\\) parameter in the Exponential distribution will be written:\n\\[\n\\begin{equation}\nGa(\\lambda | a,b)=\\frac{b^a \\lambda^{a-1} \\exp\\{-b\\lambda\\}}{\\Gamma(a)}\n\\end{equation}\n\\]\nFind the parameters \\(a^′\\) and \\(b^′\\) of the posterior distribution.\n\n\\[\n\\begin{aligned}\np(\\lambda | X) &\\propto p(X |\\lambda) p(\\lambda) \\\\\np(\\lambda | X) &\\propto \\lambda^n \\exp \\left\\{-\\lambda \\sum_{i=1}^n x_i  \\right\\}\\lambda^{a-1} \\exp\\{-b\\lambda\\} \\propto \\lambda^{a+n-1} \\exp \\left\\{-\\lambda \\left(\\sum_{i=1}^n x_i + b\\right)\\right\\}\\\\\np(\\lambda | X) &= Ga(a^′,b^′) = Ga\\left(a+n,b+\\sum_{i=1}^n x_i \\right)\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.5-conjugate-forms-4",
    "href": "chapter02.html#exercise-2.5-conjugate-forms-4",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.5 Conjugate forms 4",
    "text": "Exercise 2.5 Conjugate forms 4\n\nComputing the posterior\nThis is a contrived example. Suppose we are modeling the number of times that a speaker says the word “I” per day. This could be of interest if we are studying, for example, how self-oriented a speaker is. The number of times\n\\(x\\) that the word is uttered in over a particular time period (here, one day) can be modeled by a Poisson distribution (\\(x=0,1,2,…\\)):\n\\[\n\\begin{equation}\nf(x\\mid \\theta) = \\frac{\\exp(-\\theta) \\theta^x}{x!}\n\\end{equation}\n\\]\nwhere the rate \\(θ\\) is unknown, and the numbers of utterances of the target word on each day are independent given \\(θ\\).\nWe are told that the prior mean of \\(θ\\) is 100 and prior variance for \\(θ\\) is 225. This information is based on the results of previous studies on the topic. We will use the Gamma(a,b) density (see previous question) as a prior for\n\\(θ\\) because this is a conjugate prior to the Poisson distribution.\n\nFirst, visualize the prior, a Gamma density prior for \\(\\theta\\) based on the above information.\n\n[Hint: we know that for a Gamma density with parameters a, b, the mean is \\(\\frac{a}{b}\\) and the variance is \\(\\frac{a}{b^2}\\). Since we are given values for the mean and variance, we can solve for a,b, which gives us the Gamma density.]\n\n\\[\n\\left.\n\\begin{array}{l}\n\\frac{a}{b} = 100 \\\\\n\\frac{a}{b^2} = 225\n\\end{array}\n\\right\\}\n\\Longrightarrow\n\\left.\n\\begin{array}{l}\na= \\frac{100 \\times 100}{225} \\\\\nb= \\frac{100}{225}\n\\end{array}\n\\right\\}\n\\]\n\n\n\n\n\n\n\nNext, derive the posterior distribution of the parameter \\(θ\\) up to proportionality, and write down the posterior distribution in terms of the parameters of a Gamma distribution.\n\n\n\\[\n\\begin{aligned}\np(\\theta | X) &\\propto p(X |\\theta) p(\\theta) \\\\\np(\\theta | X) &\\propto \\prod_{i=1}^{n} \\exp(-\\theta) \\theta^{x_i} \\times \\theta^{a-1} \\exp\\{-b\\theta\\} \\\\\n               &\\propto \\exp(-n\\theta)\\theta^{\\sum_{i=1}^{n}x_i}\\theta^{a-1} \\exp\\{-b\\theta\\}\\\\\n               &\\propto \\exp(-(n+b)\\theta)\\theta^{a-1+\\sum_{i=1}^{n}x_i}\\\\\np(\\theta | X) &= Ga(a^*,b^*) = Ga\\left(a + \\sum_{i=1}^n x_i, b + n \\right),\n\\end{aligned}\n\\]\n\n\n\nPractical application\nSuppose we know that the number of “I” utterances from a particular individual is \\(115,97,79,131\\). Use the result you derived above to obtain the posterior distribution. In other words, write down the parameters of the Gamma distribution (call them \\(a^∗\\),\\(b^∗\\)) representing the posterior distribution of \\(θ\\) .\n\n\n\n\n\nNow suppose you get one new data point: 200. Using the posterior \\(Gamma(a^∗,b^∗)\\) as your prior, write down the updated posterior (in terms of the updated parameters of the Gamma distribution) given this new data point. Add the updated posterior to the plot you made above.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.6-the-posterior-mean-is-a-weighted-mean-of-the-prior-mean-and-the-mle-poisson-gamma-conjugate-case",
    "href": "chapter02.html#exercise-2.6-the-posterior-mean-is-a-weighted-mean-of-the-prior-mean-and-the-mle-poisson-gamma-conjugate-case",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.6 The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)",
    "text": "Exercise 2.6 The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)\nThe number of times an event happens per unit time can be modeled using a Poisson distribution, whose PMF is:\n\\[\n\\begin{equation}\nf(x\\mid \\theta) = \\frac{\\exp(-\\theta) \\theta^x}{x!}\n\\end{equation}\n\\]\nSuppose that we define a Gamma(a,b) prior for the rate parameter \\(θ\\). It is a fact (see exercises above) that the posterior of the \\(θ\\) parameter is a \\(Gamma(a^∗,b^∗)\\) distribution, where \\(a^∗\\) and \\(b^∗\\) are the updated parameters given the data: \\(θ∼Gamma(a^∗,b^∗)\\).\n\nProve that the posterior mean is a weighted mean of the prior mean and the maximum likelihood estimate (mean) of the Poisson-distributed data, \\(\\bar{x} = \\sum_{i=1}^n x/n\\). Hint: the mean of a Gamma distribution is \\(\\frac{a}{b}\\).\n\nSpecifically, what you have to prove is that:\n\\[\n\\frac{a^*}{b^*} = \\frac{a}{b} \\times \\frac{w_1}{w_1 + w_2} + \\bar{x} \\times \\frac{w_2}{w_1 + w_2}\n\\tag{2.1}\\]\nwhere \\(w_1=1\\) and \\(w_2=\\frac{n}{b}\\)\n\n\\[\n\\begin{aligned}\np(\\theta | X) &= Ga(a^*,b^*) = Ga\\left(a + \\sum_{i=1}^n x_i, b + n \\right) \\\\\n\\mathbb{E}[\\theta|X] &= \\frac{a^*}{b^*} = \\frac{a + \\sum_{i=1}^n x_i}{b + n}\\\\\n& = \\frac{\\frac{a}{b}+\\bar{x}\\frac{n}{b}}{1+\\frac{n}{b}} = \\\\\n&= \\frac{a}{b}\\frac{1}{1+\\frac{n}{b}}+\\bar{x}\\frac{\\frac{n}{b}}{1+\\frac{n}{b}}\n\\end{aligned}\n\\]\n\n\nGiven equation Equation 2.1, show that as \\(n\\) increases (as sample size goes up), the maximum likelihood estimate \\(\\bar{x}\\) dominates in determining the posterior mean, and when \\(n\\) gets smaller and smaller, the prior mean dominates in determining the posterior mean.\n\n\n\\[\n\\begin{aligned}\n\\lim_{n \\to \\infty}\\mathbb{E}[\\theta|X] &= \\bar{x}\\\\\n\\lim_{n \\to 0}\\mathbb{E}[\\theta|X] &= \\frac{a}{b}\\\\\n\\end{aligned}\n\\]\n\n\nFinally, given that the variance of a Gamma distribution is \\(\\frac{a}{b^2}\\), show that as \\(n\\) increases, the posterior variance will get smaller and smaller (the uncertainty on the posterior will go down).\n\n\n\\[\n\\begin{aligned}\n\\mathbb{V}[\\theta|X] &= \\frac{a*}{b*^2} = \\frac{a + \\sum_{i=1}^n x_i}{(b + n)^2} = \\frac{a + n\\bar{x}}{(b + n)^2}\\\\\n\\lim_{n \\to \\infty}\\mathbb{V}[\\theta|X] &= 0\\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.1-check-for-parameter-recovery-in-a-linear-model-using-simulated-data.",
    "href": "chapter03.html#exercise-3.1-check-for-parameter-recovery-in-a-linear-model-using-simulated-data.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.1 Check for parameter recovery in a linear model using simulated data.",
    "text": "Exercise 3.1 Check for parameter recovery in a linear model using simulated data.\nGenerate some simulated independent and identically distributed data with \\(n=100\\) data points as follows:\nNext, fit a simple linear model with a normal likelihood:\n\\[\n\\begin{equation}\ny_n  \\sim \\mathit{Normal}(\\mu,\\sigma) \\tag{3.12}\n\\end{equation}\n\\]\nSpecify the following priors:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mu &\\sim \\mathit{Uniform}(0, 60000) \\\\\n\\sigma &\\sim \\mathit{Uniform}(0, 2000)\n\\end{aligned}\n\\end{equation}\n\\]\nGenerate posterior predictive distributions of the parameters and check that the true values of the parameters \\(μ=500\\), \\(σ=50\\) are recovered by the model. What this means is that you should check whether these true values lie within the range of the posterior distributions of the two parameters. This is a good sanity check for finding out whether a model can in principle recover the true parameter values correctly.\n\n\nfit3.1 &lt;-\n  brm(y ~ 1,\n      data = data.frame(y),\n      family = gaussian(),\n      prior = c(\n        prior(uniform(0, 60000), class = Intercept, lb = 0, ub = 60000),\n        prior(uniform(0, 2000), class = sigma, lb = 0, ub = 2000)\n      ),\n      chains = 4,\n      iter = 2000,\n      warmup = 1000,\n      file_refit = \"on_change\",\n      file = \"fits/fit3.1\",\n      save_model = \"models/model3.1.stan\"\n  )\n\n\n\n[1] \"mean\"\n\n\n    2.5%      50%    97.5% \n499.9399 510.4989 521.1880 \n\n\n\n\n[1] \"sigma\"\n\n\n    2.5%      50%    97.5% \n45.29420 52.01143 59.85601",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.2-a-simple-linear-model.",
    "href": "chapter03.html#exercise-3.2-a-simple-linear-model.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.2 A simple linear model.",
    "text": "Exercise 3.2 A simple linear model.\n\nFit the model fit_press with just a few iterations, say 50 iterations (set warmup to the default of 25, and use four chains). Does the model converge?\n\n\n\nfit3.2.a &lt;-\n  brm(t ~ 1,\n      data = df_spacebar,\n      family = gaussian(),\n      prior = c(\n        prior(uniform(0, 60000), class = Intercept, lb = 0, ub = 60000),\n        prior(uniform(0, 2000), class = sigma, lb = 0, ub = 2000)\n      ),\n      chains = 4,\n      iter = 50,\n      warmup = 25,\n      file_refit = \"on_change\",\n      file = \"fits/fit3.2.a\",\n      save_model = \"models/model3.2.a.stan\"\n  )\n\nmcmc_plot(fit3.2.a, type=\"trace\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5%\n50%\n97.5%\n\n\n\n\nIntercept\n6.12\n168.04\n444.58\n\n\nsigma\n22.17\n34.09\n783.30\n\n\n\n\n\n\n\n\n\nUsing normal distributions, choose priors that better represent your assumptions/beliefs about finger tapping times. To think about a reasonable set of priors for \\(μ\\) and \\(σ\\), you should come up with your own subjective assessment about what you think a reasonable range of values can be for \\(μ\\) and how much variability might happen. There is no correct answer here, we’ll discuss priors in depth in chapter 6. Fit this model to the data. Do the posterior distributions change?\n\n\n\nfit3.2.b &lt;-\n  brm(t ~ 1,\n      data = df_spacebar,\n      family = gaussian(),\n      prior = c(\n        prior(normal(400, 100), class = Intercept),\n        prior(normal(50, 20), class = sigma)\n      ),\n      chains = 4,\n      iter = 2000,\n      warmup = 1000,\n      file_refit = \"on_change\",\n      file = \"fits/fit3.2.b\",\n      save_model = \"models/model3.2.b.stan\"\n  )\n\nmcmc_plot(fit3.2.b, type=\"trace\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5%\n50%\n97.5%\n\n\n\n\nIntercept\n166.05\n168.77\n171.21\n\n\nsigma\n23.37\n25.04\n26.99",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.3-revisiting-the-button-pressing-example-with-different-priors.",
    "href": "chapter03.html#exercise-3.3-revisiting-the-button-pressing-example-with-different-priors.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.3 Revisiting the button-pressing example with different priors.",
    "text": "Exercise 3.3 Revisiting the button-pressing example with different priors.\n\nCan you come up with very informative priors that influence the posterior in a noticeable way (use normal distributions for priors, not uniform priors)? Again, there are no correct answers here; you may have to try several different priors before you can noticeably influence the posterior.\n\n\n\nfit3.3.a &lt;-\n  brm(t ~ 1,\n      data = df_spacebar,\n      family = gaussian(),\n      prior = c(\n        prior(normal(10000, 10), class = Intercept),\n        prior(normal(10, 1), class = sigma)\n      ),\n      chains = 4,\n      iter = 2000,\n      warmup = 1000,\n      file_refit = \"on_change\",\n      file = \"fits/fit3.3.a\",\n      save_model = \"models/model3.3.a.stan\"\n  )\n\n\n\n\n\n\n\n\n\n\n2.5%\n50%\n97.5%\n\n\n\n\nIntercept\n8,103.79\n8,122.28\n8,139.88\n\n\nsigma\n389.96\n391.03\n392.11\n\n\n\n\n\n\n\n\n\nGenerate and plot prior predictive distributions based on this prior and plot them.\n\n\n\n\n\n\n\nGenerate posterior predictive distributions based on this prior and plot them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.4-posterior-predictive-checks-with-a-log-normal-model.",
    "href": "chapter03.html#exercise-3.4-posterior-predictive-checks-with-a-log-normal-model.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.4 Posterior predictive checks with a log-normal model.",
    "text": "Exercise 3.4 Posterior predictive checks with a log-normal model.\n\nFor the log-normal model fit_press_ln, change the prior of \\(σ\\) so that it is a log-normal distribution with location (\\(μ\\)) of \\(−2\\) and scale (\\(σ\\)) of \\(0.5\\). What does such a prior imply about your belief regarding button-pressing times in milliseconds? Is it a good prior? Generate and plot prior predictive distributions. Do the new estimates change compared to earlier models when you fit the model?\n\n\n\n\n\n\n\n\n\n\n\n\nFor the log-normal model, what is the mean (rather than median) time that takes to press the space bar, what is the standard deviation of the finger tapping times in milliseconds?\n\n\n\nmean [ms]   sd [ms] \n    170.1      23.0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.5-a-skew-normal-distribution.",
    "href": "chapter03.html#exercise-3.5-a-skew-normal-distribution.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.5 A skew normal distribution.",
    "text": "Exercise 3.5 A skew normal distribution.\nWould it make sense to use a “skew normal distribution” instead of the log-normal? The skew normal distribution has three parameters: location \\(ξ\\) (this is the lower-case version of the Greek letter \\(Ξ\\), pronounced “chi”, with the “ch” pronounced like the “ch” in “Bach”), scale \\(ω\\) (omega), and shape \\(α\\). The distribution is right skewed if \\(α &gt; 0\\), is left skewed if \\(α&lt;0\\), and is identical to the regular normal distribution if \\(α=0\\). For fitting this in brms, one needs to change family and set it to skew_normal(), and add a prior of class = alpha (location remains class = Intercept and scale, class = sigma).\n\nFit this model with a prior that assigns approximately 95% of the prior probability of alpha to be between 0 and 10.\n\n\\[\n\\left.\n\\begin{array}{r}\n\\alpha \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\nPr(0 \\le \\alpha \\le 10) = .95 \\\\\nPr \\left(\\frac{0-\\mu}{\\sigma} \\le \\mathcal{N}(0,1) \\le \\frac{10-\\mu}{\\sigma} \\right) = .95\n\\end{array}\n\\right\\}\n\\Longrightarrow\n\\left.\n\\begin{array}{l}\n\\frac{0-\\mu}{\\sigma} = -1.96 \\\\\n\\frac{10-\\mu}{\\sigma} = 1.96\n\\end{array}\n\\right\\}\n\\Longrightarrow\n\\left.\n\\begin{array}{l}\n\\mu = 5\\\\\n\\sigma = 5/1.96\n\\end{array}\n\\right\\}\n\\]\n\nGenerate posterior predictive distributions and compare the posterior distribution of summary statistics of the skew normal with the normal and log-normal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstat\nintercept\nsigma\n\n\nnormal\nlog normal\nskew normal\nnormal\nlog normal\nskew normal\n\n\n\n\nmean\n168.71\n170.10\n169.67\n25.07\n23.00\n24.04\n\n\n2.5%\n166.05\n167.76\n167.24\n23.37\n21.32\n22.20\n\n\n97.5%\n171.21\n172.50\n172.24\n26.99\n24.80\n26.03",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter04.html#exercise-4.1-a-simple-linear-regression-power-posing-and-testosterone.",
    "href": "chapter04.html#exercise-4.1-a-simple-linear-regression-power-posing-and-testosterone.",
    "title": "Exercises chapter 4",
    "section": "Exercise 4.1 A simple linear regression: Power posing and testosterone.",
    "text": "Exercise 4.1 A simple linear regression: Power posing and testosterone.\nLoad the following data set:\n\ndata(\"df_powerpose\")\nhead(df_powerpose) |&gt; gt()\n\n\n\n\n\n\n\nid\nhptreat\nfemale\nage\ntestm1\ntestm2\n\n\n\n\n29\nHigh\nMale\n19\n38.725\n62.375\n\n\n30\nLow\nFemale\n20\n32.770\n29.235\n\n\n31\nHigh\nFemale\n20\n32.320\n27.510\n\n\n32\nLow\nFemale\n18\n17.995\n28.655\n\n\n34\nLow\nFemale\n21\n73.580\n44.670\n\n\n35\nHigh\nFemale\n20\n80.695\n105.485\n\n\n\n\n\n\n\nThe research hypothesis is that on average, assigning a subject a high power pose vs. a low power pose will lead to higher testosterone levels after treatment. Assuming that you know nothing about typical ranges of testosterone using salivary measurement, you can use the default priors in brms for the target parameter(s).\nInvestigate this claim using a linear model and the default priors of brms. You’ll need to estimate the effect of a new variable that encodes the change in testosterone.\nThe data set, which was originally published in Carney, Cuddy, and Yap (2010) but released in modified form by Fosse (2016), shows the testosterone levels of 39 different individuals, before and after treatment, where treatment refers to each individual being assigned to a high power pose or a low power pose. In the original paper by Carney, Cuddy, and Yap (2010), the unit given for testosterone measurement (estimated from saliva samples) was picograms per milliliter (pg/ml). One picogram per milliliter is 0.001 nanogram per milliliter (ng/ml).\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4.37\n4.61\n−4.77\n13.26\n\n\nhptreatLow\n−8.70\n6.78\n−21.98\n4.59\n\n\nsd__Observation\n20.51\n2.37\n16.42\n25.74",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exercises chapter 4</span>"
    ]
  },
  {
    "objectID": "chapter04.html#exercise-4.2-another-linear-regression-model-revisiting-attentional-load-effect-on-pupil-size.",
    "href": "chapter04.html#exercise-4.2-another-linear-regression-model-revisiting-attentional-load-effect-on-pupil-size.",
    "title": "Exercises chapter 4",
    "section": "Exercise 4.2 Another linear regression model: Revisiting attentional load effect on pupil size.",
    "text": "Exercise 4.2 Another linear regression model: Revisiting attentional load effect on pupil size.\nHere, we revisit the analysis shown in the chapter, on how attentional load affects pupil size.\n\nOur priors for this experiment were quite arbitrary. How do the prior predictive distributions look like? Do they make sense?\n\n\n\n\n\n\n\nIs our posterior distribution sensitive to the priors that we selected? Perform a sensitivity analysis to find out whether the posterior is affected by our choice of prior for the \\(σ\\).\n\n\n\n\n\n\n\n\n\nTerm\nmodel 4.2.a\nmodel 4.2.b\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_Intercept\n701.27\n19.96\n660.99\n740.25\n700.77\n19.29\n662.47\n738.24\n\n\nb_c_load\n33.84\n11.93\n10.66\n57.94\n33.76\n11.71\n10.74\n56.45\n\n\nsigma\n128.73\n15.47\n102.59\n163.05\n125.32\n14.12\n100.90\n156.28\n\n\n\n\n\n\n\n\nOur data set includes also a column that indicates the trial number. Could it be that trial has also an effect on the pupil size? As in lm, we indicate another main effect with a + sign. How would you communicate the new results?\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n701.68\n17.31\n668.02\n735.71\n\n\nc_load\n31.95\n9.98\n11.82\n51.88\n\n\nc_trial\n−5.50\n1.48\n−8.35\n−2.54\n\n\nsd__Observation\n108.46\n12.62\n86.85\n136.24",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exercises chapter 4</span>"
    ]
  },
  {
    "objectID": "chapter04.html#exercise-4.3-log-normal-model-revisiting-the-effect-of-trial-on-finger-tapping-times.",
    "href": "chapter04.html#exercise-4.3-log-normal-model-revisiting-the-effect-of-trial-on-finger-tapping-times.",
    "title": "Exercises chapter 4",
    "section": "Exercise 4.3 Log-normal model: Revisiting the effect of trial on finger tapping times.",
    "text": "Exercise 4.3 Log-normal model: Revisiting the effect of trial on finger tapping times.\nWe continue considering the effect of trial on finger tapping times.\n\nEstimate the slowdown in milliseconds between the last two times the subject pressed the space bar in the experiment.\n\n\n\nmedian effect          2.5%         97.5% \n   0.09640319    0.07190941    0.12111526 \n\n\n\n\nmean effect        2.5%       97.5% \n 0.09713902  0.07250484  0.12203735 \n\n\n\nHow would you change your model (keeping the log-normal likelihood) so that it includes centered log-transformed trial numbers or square-root-transformed trial numbers (instead of centered trial numbers)? Does the effect in milliseconds change?\n\n\n\nmedian effect          2.5%         97.5% \n   0.02107170    0.01554976    0.02656155 \n\n\n\n\nmean effect        2.5%       97.5% \n 0.02122792  0.01567363  0.02676155",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exercises chapter 4</span>"
    ]
  },
  {
    "objectID": "chapter04.html#exercise-4.4-logistic-regression-revisiting-the-effect-of-set-size-on-free-recall.",
    "href": "chapter04.html#exercise-4.4-logistic-regression-revisiting-the-effect-of-set-size-on-free-recall.",
    "title": "Exercises chapter 4",
    "section": "Exercise 4.4 Logistic regression: Revisiting the effect of set size on free recall.",
    "text": "Exercise 4.4 Logistic regression: Revisiting the effect of set size on free recall.\nOur data set includes also a column coded as tested that indicates the position of the queued word. (In Figure 4.13 tested would be 3). Could it be that position also has an effect on recall accuracy? How would you incorporate this in the model? (We indicate another main effect with a + sign).\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n1.93\n0.31\n1.35\n2.57\n\n\nc_set_size\n−0.18\n0.08\n−0.34\n−0.01\n\n\nc_tested\n−0.03\n0.08\n−0.19\n0.14\n\n\n\n\n\n\n\n\n\n Estimate      Q2.5     Q97.5 \n0.8699825 0.7948330 0.9290336",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exercises chapter 4</span>"
    ]
  },
  {
    "objectID": "chapter04.html#exercise-4.5-red-is-the-sexiest-color.",
    "href": "chapter04.html#exercise-4.5-red-is-the-sexiest-color.",
    "title": "Exercises chapter 4",
    "section": "Exercise 4.5 Red is the sexiest color.",
    "text": "Exercise 4.5 Red is the sexiest color.\nLoad the following data set:\n\nhead(df_red) |&gt; gt()\n\n\n\n\n\n\n\nrisk\nage\nred\npink\nredorpink\n\n\n\n\n0\n19\n0\n0\n0\n\n\n0\n25\n0\n0\n0\n\n\n0\n20\n0\n0\n0\n\n\n0\n20\n0\n0\n0\n\n\n0\n20\n0\n0\n0\n\n\n0\n18\n0\n0\n0\n\n\n\n\n\n\n\nThe data set is from a study (Beall and Tracy 2013) that contains information about the color of the clothing worn (red, pink, or red or pink) when the subject (female) is at risk of becoming pregnant (is ovulating, self-reported). The broader issue being investigated is whether women wear red more often when they are ovulating (in order to attract a mate). Using logistic regressions, fit three different models to investigate whether being ovulating increases the probability of wearing (a) red, (b) pink, or (c) either pink or red. Use priors that are reasonable (in your opinion).\n\nred\n\n\\[\nred_n \\sim \\text{Bernoulli}(\\theta_n) \\\\\n\\log\\left(\\frac{\\theta}{1-\\theta}\\right) = \\alpha + \\beta \\cdot \\text{red}\n\\]\n\npink\n\n\neither pink or red\n\n\n\n\n\n\n\n\n\ncolor\n0\n1\n\n\n\n\nred\n0.073\n0.075\n\n\npink\n0.117\n0.121\n\n\neither pink or red\n0.179\n0.187",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exercises chapter 4</span>"
    ]
  },
  {
    "objectID": "chapter05.html#exercise-5.1-a-hierarchical-model-normal-likelihood-of-cognitive-load-on-pupil-size.",
    "href": "chapter05.html#exercise-5.1-a-hierarchical-model-normal-likelihood-of-cognitive-load-on-pupil-size.",
    "title": "Exercises chapter 5",
    "section": "Exercise 5.1 A hierarchical model (normal likelihood) of cognitive load on pupil size.",
    "text": "Exercise 5.1 A hierarchical model (normal likelihood) of cognitive load on pupil size.\nAs in section 4.1, we focus on the effect of cognitive load on pupil size, but this time we look at all the subjects of Wahn et al. (2016):\n\nhead(df_pupil_complete) |&gt; gt()\n\n\n\n\n\n\n\nsubj\ntrial\nload\np_size\n\n\n\n\n701\n1\n2\n1021.4086\n\n\n701\n2\n1\n951.4349\n\n\n701\n3\n5\n1063.9569\n\n\n701\n4\n4\n913.4871\n\n\n701\n5\n0\n602.6868\n\n\n701\n6\n3\n826.2199\n\n\n\n\n\n\n\nYou should be able to now fit a “maximal” model (correlated varying intercept and slopes for subjects) assuming a normal likelihood. Base your priors in the priors discussed in section 4.1.\n\nExamine the effect of load on pupil size, and the average pupil size. What do you conclude?\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_Intercept\n2,473.58\n504.44\n1,473.17\n3,420.74\n\n\nb_c_load\n39.39\n23.51\n−7.84\n82.56\n\n\n\n\n\n\n\n\nDo a sensitivity analysis for the prior on the intercept (\\(\\alpha\\)). What is the estimate of the effect (\\(\\beta\\)) under different priors?\n\n\nTODO\n\n\nIs the effect of load consistent across subjects? Investigate this visually.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercises chapter 5</span>"
    ]
  },
  {
    "objectID": "chapter05.html#exercise-5.2-are-subject-relatives-easier-to-process-than-object-relatives-log-normal-likelihood",
    "href": "chapter05.html#exercise-5.2-are-subject-relatives-easier-to-process-than-object-relatives-log-normal-likelihood",
    "title": "Exercises chapter 5",
    "section": "Exercise 5.2 Are subject relatives easier to process than object relatives (log-normal likelihood)?",
    "text": "Exercise 5.2 Are subject relatives easier to process than object relatives (log-normal likelihood)?\nWe begin with a classic question from the psycholinguistics literature: Are subject relatives easier to process than object relatives? The data come from Experiment 1 in a paper by Grodner and Gibson (2005).\nScientific question: Is there a subject relative advantage in reading?\nGrodner and Gibson (2005) investigate an old claim in psycholinguistics that object relative clause (ORC) sentences are more difficult to process than subject relative clause (SRC) sentences. One explanation for this predicted difference is that the distance between the relative clause verb (sent in the example below) and the head noun phrase of the relative clause (reporter in the example below) is longer in ORC vs. SRC. Examples are shown below. The relative clause is shown in square brackets.\n(1a) The reporter [who the photographer sent to the editor] was hoping for a good story. (ORC) (1b) The reporter [who sent the photographer to the editor] was hoping for a good story. (SRC)\nThe underlying explanation has to do with memory processes: Shorter linguistic dependencies are easier to process due to either reduced interference or decay, or both. For implemented computational models that spell this point out, see Lewis and Vasishth (2005) and Engelmann, Jäger, and Vasishth (2020).\nIn the Grodner and Gibson data, the dependent measure is reading time at the relative clause verb, (e.g., sent) of different sentences with either ORC or SRC. The dependent variable is in milliseconds and was measured in a self-paced reading task. Self-paced reading is a task where subjects read a sentence or a short text word-by-word or phrase-by-phrase, pressing a button to get each word or phrase displayed; the preceding word disappears every time the button is pressed. In 6.1, we provide a more detailed explanation of this experimental method.\nFor this experiment, we are expecting longer reading times at the relative clause verbs of ORC sentences in comparison to the relative clause verb of SRC sentences.\n\nhead(df_gg05_rc) |&gt; gt()\n\n\n\n\n\n\n\nsubj\nitem\ncondition\nRT\nresidRT\nqcorrect\nexperiment\n\n\n\n\n1\n1\nobjgap\n320\n-21.39\n0\ntedrg3\n\n\n1\n2\nsubjgap\n424\n74.66\n1\ntedrg2\n\n\n1\n3\nobjgap\n309\n-40.34\n0\ntedrg3\n\n\n1\n4\nsubjgap\n274\n-91.24\n1\ntedrg2\n\n\n1\n5\nobjgap\n333\n-8.39\n1\ntedrg3\n\n\n1\n6\nsubjgap\n266\n-87.32\n1\ntedrg2\n\n\n\n\n\n\n\nYou should use a sum coding for the predictors. Here, object relative clauses (“objgaps”) are coded \\(+1\\), subject relative clauses \\(−1\\).\n\ndf_gg05_rc &lt;- df_gg05_rc |&gt;\n  mutate(c_cond = if_else(condition == \"objgap\", 1, -1))\n\nYou should be able to now fit a “maximal” model (correlated varying intercept and slopes for subjects and for items) assuming a log-normal likelihood.\n\nExamine the effect of relative clause attachment site (the predictor c_cond) on reading times RT (\\(\\beta\\)).\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_c_cond\n0.01\n0.01\n−0.01\n0.03\n\n\n\n\n\n\n\n\nEstimate the median difference between relative clause attachment sites in milliseconds, and report the mean and 95% CI.\n\n\n\n\n\n\n\n\n\nmean\nQ2.5\nQ97.5\n\n\n\n\n6.63\n−5.69\n19.59\n\n\n\n\n\n\n\n\nDo a sensitivity analysis. What is the estimate of the effect (\\(\\beta\\)) under different priors? What is the difference in milliseconds between conditions under different priors?\n\n\nTODO",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercises chapter 5</span>"
    ]
  },
  {
    "objectID": "chapter05.html#exercise-5.3-relative-clause-processing-in-mandarin-chinese",
    "href": "chapter05.html#exercise-5.3-relative-clause-processing-in-mandarin-chinese",
    "title": "Exercises chapter 5",
    "section": "Exercise 5.3 Relative clause processing in Mandarin Chinese",
    "text": "Exercise 5.3 Relative clause processing in Mandarin Chinese\nLoad the following two data sets:\n\ndata(\"df_gibsonwu\")\ndata(\"df_gibsonwu2\")\n\nThe data are taken from two experiments that investigate (inter alia) the effect of relative clause type on reading time in Chinese. The data are from Gibson and Wu (2013) and Vasishth et al. (2013) respectively. The second data set is a direct replication attempt of the Gibson and Wu (2013) experiment.\nChinese relative clauses are interesting theoretically because they are prenominal: the relative clause appears before the head noun. For example, the English relative clauses shown above would appear in the following order in Mandarin. The square brackets mark the relative clause, and REL refers to the Chinese equivalent of the English relative pronoun who.\n(2a) [The photographer sent to the editor] REL the reporter was hoping for a good story. (ORC) (2b) [sent the photographer to the editor] REL the reporter who was hoping for a good story. (SRC)\nAs discussed in Gibson and Wu (2013), the consequence of Chinese relative clauses being prenominal is that the distance between the verb in relative clause and the head noun is larger in subject relatives than object relatives. Hsiao and Gibson (2003) were the first to suggest that the larger distance in subject relatives leads to longer reading time at the head noun. Under this view, the prediction is that subject relatives are harder to process than object relatives. If this is true, this is interesting and surprising because in most other languages that have been studied, subject relatives are easier to process than object relatives; so Chinese will be a very unusual exception cross-linguistically.\nThe data provided are for the critical region (the head noun; here, reporter). The experiment method is self-paced reading, so we have reading times in milliseconds. The second data set is a direct replication attempt of the first data set, which is from Gibson and Wu (2013).\nThe research hypothesis is whether the difference in reading times between object and subject relative clauses is negative. For the first data set (df_gibsonwu), investigate this question by fitting two “maximal” hierarchical models (correlated varying intercept and slopes for subjects and items). The dependent variable in both models is the raw reading time in milliseconds. The first model should use the normal likelihood in the model; the second model should use the log-normal likelihood. In both models, use \\(\\pm 0.5\\) sum coding to model the effect of relative clause type. You will need to decide on appropriate priors for the various parameters.\n\nPlot the posterior predictive distributions from the two models. What is the difference in the posterior predictive distributions of the two models; and why is there a difference?\nExamine the posterior distributions of the effect estimates (in milliseconds) in the two models. Why are these different?\nGiven the posterior predictive distributions you plotted above, why is the log-normal likelihood model better for carrying out inference and hypothesis testing?\n\nNext, work out a normal approximation of the log-normal model’s posterior distribution for the relative clause effect that you obtained from the above data analysis. Then use that normal approximation as an informative prior for the slope parameter when fitting a hierarchical model to the second data set. This is an example of incrementally building up knowledge by successively using a previous study’s posterior as a prior for the next study; this is essentially equivalent to pooling both data sets (check that pooling the data and using a Normal(0,1) prior for the effect of interest, with a log-normal likelihood, gives you approximately the same posterior as the informative-prior model fit above).\n\nTODO",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercises chapter 5</span>"
    ]
  },
  {
    "objectID": "chapter05.html#exercise-5.4-agreement-attraction-in-comprehension",
    "href": "chapter05.html#exercise-5.4-agreement-attraction-in-comprehension",
    "title": "Exercises chapter 5",
    "section": "Exercise 5.4 Agreement attraction in comprehension",
    "text": "Exercise 5.4 Agreement attraction in comprehension\n\nhead(df_dillonE1) |&gt; gt()\n\n\n\n\n\n\n\nsubj\nitem\nrt\nint\nexpt\n\n\n\n\ndillonE11\ndillonE119\n2918\nlow\ndillonE1\n\n\ndillonE11\ndillonE119\n1338\nlow\ndillonE1\n\n\ndillonE11\ndillonE119\n424\nlow\ndillonE1\n\n\ndillonE11\ndillonE119\n186\nlow\ndillonE1\n\n\ndillonE11\ndillonE119\n195\nlow\ndillonE1\n\n\ndillonE11\ndillonE119\n1218\nlow\ndillonE1\n\n\n\n\n\n\n\nThe data are taken from an experiment that investigate (inter alia) the effect of number similarity between a noun and the auxiliary verb in sentences like the following. There are two levels to a factor called Int(erference): low and high.\n(3a) low: The key to the cabinet are on the table (3b) high: The key to the cabinets are on the table\nHere, in (3b), the auxiliary verb are is predicted to be read faster than in (3a), because the plural marking on the noun cabinets leads the reader to think that the sentence is grammatical. (Both sentences are ungrammatical.) This phenomenon, where the high condition is read faster than the low condition, is called agreement attraction.\nThe data provided are for the critical region (the auxiliary verb are). The experiment method is eye-tracking; we have total reading times in milliseconds.\nThe research question is whether the difference in reading times between high and low conditions is negative.\n\nFirst, using a log-normal likelihood, fit a hierarchical model with correlated varying intercept and slopes for subjects and items. You will need to decide on the priors for the model.\nBy simply looking at the posterior distribution of the slope parameter \\(\\beta\\), what would you conclude about the theoretical claim relating to agreement attraction?\n\n\nTODO",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercises chapter 5</span>"
    ]
  },
  {
    "objectID": "chapter05.html#exercise-5.5-attentional-blink-bernoulli-likelihood",
    "href": "chapter05.html#exercise-5.5-attentional-blink-bernoulli-likelihood",
    "title": "Exercises chapter 5",
    "section": "Exercise 5.5 Attentional blink (Bernoulli likelihood)",
    "text": "Exercise 5.5 Attentional blink (Bernoulli likelihood)\nThe attentional blink (AB; first described by Raymond, Shapiro, and Arnell 1992; though it has been noticed before e.g., Broadbent and Broadbent 1987) refers to a temporary reduction in the accuracy of detecting a probe (e.g., a letter “X”) presented closely after a target that has been detected (e.g., a white letter). We will focus on the experimental condition of Experiment 2 of Raymond, Shapiro, and Arnell (1992). Subjects are presented with letters in rapid serial visual presentation (RSVP) at the center of the screen at a constant rate and are required to identify the only white letter (target) in the stream of black letters, and then to report whether the letter X (probe) occurred in the subsequent letter stream. The AB is defined as having occurred when the target is reported correctly but the report of the probe is inaccurate at a short lag or target-probe interval.\nThe data set df_ab is a subset of the data of this paradigm from a replication conducted by Grassi et al. (2021). In this subset, the probe was always present and the target was correctly identified. We want to find out how the lag affects the accuracy of the identification of the probe.\n\nhead(df_ab) |&gt; gt()\n\n\n\n\n\n\n\nsubj\nprobe_correct\ntrial\nlag\n\n\n\n\n1\n0\n2\n5\n\n\n1\n1\n4\n4\n\n\n1\n1\n8\n6\n\n\n1\n0\n14\n5\n\n\n1\n0\n15\n2\n\n\n1\n0\n18\n8\n\n\n\n\n\n\n\nFit a logistic regression assuming a linear relationship between lag and accuracy (probe_correct). Assume a hierarchical structure with correlated varying intercept and slopes for subjects. You will need to decide on the priors for this model.\n\nHow is the accuracy of the probe identification affected by the lag? Estimate this in log-odds and percentages.\n\n\n\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nb_Intercept\n0.29\n0.16\n0.00\n0.62\n\n\nb_c_lag\n0.01\n0.02\n−0.04\n0.06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlag\nc_lag\nprob\n\n\n\n\n0.00\n−3.99\n0.56\n\n\n1.00\n−2.99\n0.56\n\n\n2.00\n−1.99\n0.57\n\n\n3.00\n−0.99\n0.57\n\n\n4.00\n0.01\n0.57\n\n\n5.00\n1.01\n0.58\n\n\n6.00\n2.01\n0.58\n\n\n7.00\n3.01\n0.58\n\n\n8.00\n4.01\n0.58\n\n\n\n\n\n\n\n\nIs the linear relationship justified? Use posterior predictive checks to verify this.\n\n\n\n\n\n\n\nCan you think about a better relationship between lag and accuracy? Fit a new model and use posterior predictive checks to verify if the fit improved.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercises chapter 5</span>"
    ]
  },
  {
    "objectID": "chapter08.html#exercise-8.1-contrast-coding-for-a-four-condition-design",
    "href": "chapter08.html#exercise-8.1-contrast-coding-for-a-four-condition-design",
    "title": "Exercises chapter 8",
    "section": "Exercise 8.1 Contrast coding for a four-condition design",
    "text": "Exercise 8.1 Contrast coding for a four-condition design\nLoad the following data. These data are from Experiment 1 in a set of reading studies on Persian (Safavi, Husain, and Vasishth 2016). This is a self-paced reading study on particle-verb constructions, with a \\(2×2\\) design: distance (short, long) and predictability (predictable, unpredictable). The data are from a critical region in the sentence. All the data from the Safavi, Husain, and Vasishth (2016) paper are available from https://github.com/vasishth/SafaviEtAl2016.\n\n\n\n\n\n\n\n\nsubj\nitem\nrt\ndistance\npredability\n\n\n\n\n4\n6\n568\nshort\npredictable\n\n\n4\n17\n517\nlong\nunpredictable\n\n\n4\n22\n675\nshort\npredictable\n\n\n4\n5\n575\nlong\nunpredictable\n\n\n4\n3\n581\nlong\npredictable\n\n\n4\n7\n1171\nlong\npredictable\n\n\n\n\n\n\n\nThe four conditions are:\n\nDistance=short and Predictability=unpredictable\nDistance=short and Predictability=predictable\nDistance=long and Predictability=unpredictable\nDistance=long and Predictability=predictable\n\nThe researcher wants to do the following sets of comparisons between condition means:\nCompare the condition labeled Distance=short and Predictability=unpredictable with each of the following conditions:\n\nDistance=short and Predictability=predictable\nDistance=long and Predictability=unpredictable\nDistance=long and Predictability=predictable\n\nQuestions:\n\nWhich contrast coding is needed for such a comparison?\nFirst, define the relevant contrast coding. Hint: You can do it by creating a condition column labeled a,b,c,d and then use a built-in contrast coding function.\nThen, use the hypr library function to confirm that your contrast coding actually does the comparison you need.\nFit a simple linear model with the above contrast coding and display the slopes, which constitute the relevant comparisons.\nNow, compute each of the four conditions’ means and check that the slopes from the linear model correspond to the relevant differences between means that you obtained from the data.\n\n\n\n\n\n\n\n\n\nhypothesis\nValue\n\n\n\n\nH0\n575.841270\n\n\nH1\n-40.111111\n\n\nH2\n69.743386\n\n\nH3\n1.780423\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef\nValue\n\n\n\n\n(Intercept)\n575.841270\n\n\nconditionshort.predictable\n-40.111111\n\n\nconditionlong.unpredictable\n69.743386\n\n\nconditionlong.predictable\n1.780423\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef\nValue\n\n\n\n\n(Intercept)\n575.841270\n\n\nconditionH1\n-40.111111\n\n\nconditionH2\n69.743386\n\n\nconditionH3\n1.780423\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef\nValue\n\n\n\n\n(Intercept)\n575.841270\n\n\nconditionH1\n-40.111111\n\n\nconditionH2\n69.743386\n\n\nconditionH3\n1.780423",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exercises chapter 8</span>"
    ]
  },
  {
    "objectID": "chapter08.html#exercise-8.2-helmert-coding-for-a-four-condition-design.",
    "href": "chapter08.html#exercise-8.2-helmert-coding-for-a-four-condition-design.",
    "title": "Exercises chapter 8",
    "section": "Exercise 8.2 Helmert coding for a four-condition design.",
    "text": "Exercise 8.2 Helmert coding for a four-condition design.\nLoad the following data:\n\n\n\n\n\n\n\n\nsubject\nitem\ncondition\ntimes\nvalue\n\n\n\n\n1\n6\nf\nSFD\n327.845\n\n\n1\n24\nf\nSFD\n205.948\n\n\n1\n35\ne\nSFD\n315.225\n\n\n1\n17\ne\nSFD\n264.773\n\n\n1\n34\nd\nSFD\n252.193\n\n\n1\n7\na\nSFD\n155.511\n\n\n\n\n\n\n\nThe data come from an eyetracking study in German reported in Vasishth et al. (2008). The experiment is a reading study involving six conditions. The sentences are in English, but the original design was involved German sentences. In German, the word durchaus (certainly) is a positive polarity item: in the constructions used in this experiment, durchaus cannot have a c-commanding element that is a negative polarity item licensor. Here are the conditions:\n\nNegative polarity items\n\nGrammatical: No man who had a beard was ever thrifty.\nUngrammatical (Intrusive NPI licensor): A man who had no beard was ever thrifty.\nUngrammatical: A man who had a beard was ever thrifty.\n\nPositive polarity items\n\nUngrammatical: No man who had a beard was certainly thrifty.\nGrammatical (Intrusive NPI licensor): A man who had no beard was certainly thrifty.\nGrammatical: A man who had a beard was certainly thrifty.\n\n\nWe will focus only on re-reading time in this data set. Subset the data so that we only have re-reading times in the data frame:\n\n\n\n\n\n\n\n\nsubject\nitem\ncondition\ntimes\nvalue\n\n\n\n\n1\n20\nb\nRRT\n239.571\n\n\n1\n3\nc\nRRT\n1866.169\n\n\n1\n13\na\nRRT\n529.576\n\n\n1\n19\na\nRRT\n269.002\n\n\n1\n27\nc\nRRT\n844.770\n\n\n1\n26\nb\nRRT\n634.654\n\n\n\n\n\n\n\nThe comparisons we are interested in are:\n\nWhat is the difference in reading time between negative polarity items and positive polarity items?\nWithin negative polarity items, what is the difference between grammatical and ungrammatical conditions?\nWithin negative polarity items, what is the difference between the two ungrammatical conditions?\nWithin positive polarity items, what is the difference between grammatical and ungrammatical conditions?\nWithin positive polarity items, what is the difference between the two grammatical conditions?\n\nUse the hypr package to specify the comparisons specified above, and then extract the contrast matrix. Finally, specify the contrasts to the condition column in the data frame. Fit a linear model using this contrast specification, and then check that the estimates from the model match the mean differences between the conditions being compared.\n\n\n\n\n\n\n\n\nhypothesis\nvalue\n\n\n\n\nnegative - positive\n152.01071\n\n\nnegative.grammatical - negative.ungrammatical\n-152.15798\n\n\nnegative.ungrammatical diff\n-25.42387\n\n\npositive.grammatical - positive.ungrammatical\n-139.82495\n\n\npositive.grammatical.1 - positive.grammatical.2\n34.02527\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef\nValue\n\n\n\n\n(Intercept)\n511.72170\n\n\nconditionnegative - positive\n153.10226\n\n\nconditionnegative.grammatical - negative.ungrammatical\n-152.41591\n\n\nconditionnegative.ungrammatical diff\n-25.42387\n\n\nconditionpositive.grammatical - positive.ungrammatical\n-140.88824\n\n\ncondition\n-24.05950\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef\nValue\n\n\n\n\n(Intercept)\n511.72170\n\n\nconditionnegative - positive\n153.10226\n\n\nconditionnegative.grammatical - negative.ungrammatical\n-152.41591\n\n\nconditionnegative.ungrammatical diff\n-25.42387\n\n\nconditionpositive.grammatical - positive.ungrammatical\n-140.88824\n\n\nconditionpositive.grammatical diff\n34.02527",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exercises chapter 8</span>"
    ]
  },
  {
    "objectID": "chapter08.html#exercise-8.3-number-of-possible-comparisons-in-a-single-model.",
    "href": "chapter08.html#exercise-8.3-number-of-possible-comparisons-in-a-single-model.",
    "title": "Exercises chapter 8",
    "section": "Exercise 8.3 Number of possible comparisons in a single model.",
    "text": "Exercise 8.3 Number of possible comparisons in a single model.\nHow many comparisons can one make in a single model when there is a single factor with four levels? Why can we not code four comparisons in a single model?\n\nYou can make as many comparisons as the number of levels minus one. In this case, three comparisons. The fourth comparison will be a linear combination of the other three and will result in multicollinearity.\n\nHow many comparisons can one code in a model where there are two factors, one with three levels and one with two levels?\n\nThis is equivalent to a factor with six levels (3x2). Therefore, five comparisons can be made.\n\nHow about a model for a \\(2×2×3\\) design?\n\n\\(2×2×3 - 1 = 11\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exercises chapter 8</span>"
    ]
  },
  {
    "objectID": "chapter09.html#exercise-9.1-anova-coding-for-a-four-condition-design.",
    "href": "chapter09.html#exercise-9.1-anova-coding-for-a-four-condition-design.",
    "title": "Exercises chapter 9",
    "section": "Exercise 9.1 ANOVA coding for a four-condition design.",
    "text": "Exercise 9.1 ANOVA coding for a four-condition design.\nLoad the following data. These data are from Experiment 1 in a set of reading studies on Persian (Safavi, Husain, and Vasishth 2016); we encountered these data in the preceding chapter’s exercises.\n\n\n\n\n\n\n\n\nsubj\nitem\nrt\ndistance\npredability\n\n\n\n\n4\n6\n568\nshort\npredictable\n\n\n4\n17\n517\nlong\nunpredictable\n\n\n4\n22\n675\nshort\npredictable\n\n\n4\n5\n575\nlong\nunpredictable\n\n\n4\n3\n581\nlong\npredictable\n\n\n4\n7\n1171\nlong\npredictable\n\n\n\n\n\n\n\nThe four conditions are:\nDistance=short and Predictability=unpredictable Distance=short and Predictability=predictable Distance=long and Predictability=unpredictable Distance=long and Predictability=predictable\nFor the data given above, define an ANOVA-style contrast coding, and compute main effects and interactions. Check with hypr what the estimated comparisons are with an ANOVA coding.\n\n\n\n\n\n\n\n\ngrand mean\nlong - short\npredictable - unpredictable\ninteraction\n\n\n\n\n583.69\n55.82\n−54.04\n−27.85\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef\nValue\n\n\n\n\n(Intercept)\n583.69\n\n\ndistance1\n55.82\n\n\npredability1\n−54.04\n\n\ndistance1:predability1\n−27.85",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercises chapter 9</span>"
    ]
  },
  {
    "objectID": "chapter09.html#exercise-9.2-anova-and-nested-comparisons-in-a-2-times-2-times-2-design.",
    "href": "chapter09.html#exercise-9.2-anova-and-nested-comparisons-in-a-2-times-2-times-2-design.",
    "title": "Exercises chapter 9",
    "section": "Exercise 9.2 ANOVA and nested comparisons in a \\(2 \\times 2 \\times 2\\) design.",
    "text": "Exercise 9.2 ANOVA and nested comparisons in a \\(2 \\times 2 \\times 2\\) design.\nLoad the following data set. This is a \\(2 \\times 2 \\times 2\\) design from Jäger et al. (2020), with the factors Grammaticality (grammatical vs. ungrammatical), Dependency (Agreement vs. Reflexives), and Interference (Interference vs. no interference). The experiment is a replication attempt of Experiment 1 reported in Dillon et al. (2013).\n\n\n\n\n\n\n\n\nsubj\nitem\ncond\nacc\nroi\nFPRT\nTFT\nFPR\n\n\n\n\n23\n33\nd\n-1\n12\n207\n3393\n1\n\n\n23\n32\ne\n0\n12\n311\n1165\n0\n\n\n23\n45\nh\n-1\n12\n238\n2005\n0\n\n\n23\n14\ng\n-1\n12\n303\n5231\n1\n\n\n23\n44\na\n1\n12\n520\n1312\n1\n\n\n23\n46\ng\n1\n12\n391\n521\n1\n\n\n\n\n\n\n\n\nThe grammatical conditions are a,b,e,f. The rest of the conditions are ungrammatical.\nThe agreement conditions are a,b,c,d. The other conditions are reflexives.\nThe interference conditions are a,d,e,h, and the others are the no-interference conditions.\n\nThe dependent measure of interest is TFT (total fixation time, in milliseconds).\nUsing a linear model, do a main effects and interactions ANOVA contrast coding, and obtain an estimate of the main effects of Grammaticality, Dependency, and Interference, and all interactions. You may find it easier to code the contrasts coding the main effects as +1, -1, using ifelse() in R to code vectors corresponding to each main effect. This will make the specification of the interactions easy.\n\n\n\n\n\n\n\n\nCoef\nValue\n\n\n\n\n(Intercept)\n580.60\n\n\nGrammaticality1\n−73.24\n\n\nDependency1\n73.77\n\n\nInterference1\n−3.67\n\n\nGrammaticality1:Dependency1\n0.43\n\n\nGrammaticality1:Interference1\n14.26\n\n\nDependency1:Interference1\n0.72\n\n\nGrammaticality1:Dependency1:Interference1\n0.20\n\n\n\n\n\n\n\nThe researchers had a further research hypothesis: in ungrammatical sentences only, agreement would show an interference effect but reflexives would not. In grammatical sentences, both agreement and reflexives are expected to show interference effects. This kind of research question can be answered with nested contrast coding.\nTo carry out the relevant nested contrasts, define contrasts that estimate the effects of\n\ngrammaticality\ndependency type\nthe interaction between grammaticality and dependency type\nreflexives interference within grammatical conditions\nagreement interference within grammatical conditions\nreflexives interference within ungrammatical conditions\nagreement interference within ungrammatical conditions\n\nDo the estimates match expectations? Check this by computing the condition means and checking that the estimates from the models match the relevant differences between conditions or clusters of conditions.\n\n\n\n\n\n\n\n\ncontrast\nvalue\n\n\n\n\ngrand_mean\n580.57\n\n\ngrammatical_ungrammatical\n−146.47\n\n\nagreement_reflexives\n147.51\n\n\nGrammaticality_Dependency\n3.39\n\n\nagreement_reflexives | interference, grammatical\n150.24\n\n\nagreement_reflexives | interference, ungrammatical\n147.73\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef\nValue\n\n\n\n\n(Intercept)\n580.619790\n\n\ncondgrammatical_ungrammatical\n-146.505175\n\n\ncondagreement_reflexives\n147.539493\n\n\ncondGrammaticality_Dependency\n3.382627\n\n\ncondDependency | grammatical, interference\n150.232855\n\n\ncondDependency | ungrammatical, interference\n147.749069",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exercises chapter 9</span>"
    ]
  }
]