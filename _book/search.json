[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bayescogsci exercises",
    "section": "",
    "text": "Preface\nExercise solutions of An Introduction to Bayesian Data Analysis for Cognitive Science Book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.1-practice-using-the-pnorm-functionpart-1",
    "href": "chapter01.html#exercise-1.1-practice-using-the-pnorm-functionpart-1",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.1 Practice using the pnorm() function–Part 1",
    "text": "Exercise 1.1 Practice using the pnorm() function–Part 1\nGiven a normal distribution with mean 500 and standard deviation 100, use the pnorm function to calculate the probability of obtaining values between 200 and 800 from this distribution.\n\npnorm(800, mean = 500, sd = 100) - pnorm(200, mean = 500, sd = 100) \n\n[1] 0.9973002\n\n1-2*pnorm(200, mean = 500, sd = 100)\n\n[1] 0.9973002",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.2-practice-using-the-pnorm-functionpart-2",
    "href": "chapter01.html#exercise-1.2-practice-using-the-pnorm-functionpart-2",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.2 Practice using the pnorm() function–Part 2",
    "text": "Exercise 1.2 Practice using the pnorm() function–Part 2\nCalculate the following probabilities. Given a normal distribution with mean 800 and standard deviation 150, what is the probability of obtaining:\na score of 700 or less a score of 900 or more a score of 800 or more\n\nc(pnorm(700, 800, 150), pnorm(900, 800, 150, lower.tail = FALSE), 1-pnorm(800, 800, 150))\n\n[1] 0.2524925 0.2524925 0.5000000",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.3-practice-using-the-pnorm-functionpart-3",
    "href": "chapter01.html#exercise-1.3-practice-using-the-pnorm-functionpart-3",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.3 Practice using the pnorm() function–Part 3",
    "text": "Exercise 1.3 Practice using the pnorm() function–Part 3\nGiven a normal distribution with mean 600 and standard deviation 200, what is the probability of obtaining:\na score of 550 or less. a score between 300 and 800. a score of 900 or more.\n\nc(pnorm(550, 600, 150), pnorm(800, 600, 150)-pnorm(300, 600, 150), 1-pnorm(900, 600, 150))\n\n[1] 0.36944134 0.88603865 0.02275013",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.4-practice-using-the-qnorm-functionpart1",
    "href": "chapter01.html#exercise-1.4-practice-using-the-qnorm-functionpart1",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.4 Practice using the qnorm() function–Part1",
    "text": "Exercise 1.4 Practice using the qnorm() function–Part1\nConsider a normal distribution with mean 1 and standard deviation 1. Compute the lower and upper boundaries such that:\nthe area (the probability) to the left of the lower boundary is 0.10. the area (the probability) to the right of the upper boundary is 0.90.\n\nc(qnorm(.1, 1, 1), qnorm(.9, 1, 1, lower.tail = FALSE))\n\n[1] -0.2815516 -0.2815516",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.5-practice-using-the-qnorm-functionpart2",
    "href": "chapter01.html#exercise-1.5-practice-using-the-qnorm-functionpart2",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.5 Practice using the qnorm() function–Part2",
    "text": "Exercise 1.5 Practice using the qnorm() function–Part2\nGiven a normal distribution with mean 650 and standard deviation 125. There exist two quantiles, the lower quantile q1 and the upper quantile q2, that are equidistant from the mean 650, such that the area under the curve of the normal between q1 and q2 is 80%. Find q1 and q2.\n\nx &lt;- c(qnorm(.1, 650, 125), qnorm(.1, 650, 125, lower.tail = FALSE)) \npnorm(x[2], 650, 125) - pnorm(x[1], 650, 125) \n\n[1] 0.8",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.6-practice-getting-summaries-from-samplespart-1",
    "href": "chapter01.html#exercise-1.6-practice-getting-summaries-from-samplespart-1",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.6 Practice getting summaries from samples–Part 1",
    "text": "Exercise 1.6 Practice getting summaries from samples–Part 1\nGiven data that is generated as follows:\n\ndata_gen1 &lt;- rnorm(1000, 300, 200)\n\nCalculate the mean, variance, and the lower quantile q1 and the upper quantile q2, that are equidistant and such that the range of probability between them is 80%.\n\nc(mean=mean(data_gen1), var=var(data_gen1), quantile(data_gen1, c(.1, .9)))\n\n       mean         var         10%         90% \n  300.17762 40097.00730    48.86414   570.06581",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.7-practice-getting-summaries-from-samplespart-2.",
    "href": "chapter01.html#exercise-1.7-practice-getting-summaries-from-samplespart-2.",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.7 Practice getting summaries from samples–Part 2.",
    "text": "Exercise 1.7 Practice getting summaries from samples–Part 2.\nThis time we generate the data with a truncated normal distribution from the package extraDistr. The details of this distribution will be discussed later in section 4.1 and in the Box 4.1, but for now we can treat it as an unknown generative process:\n\nlibrary(extraDistr)\ndata_gen1 &lt;- rtnorm(1000, 300, 200, a = 0)\n\nUsing the sample data, calculate the mean, variance, and the lower quantile q1 and the upper quantile q2, such that the probability of observing values between these two quantiles is 80%.\n\nc(mean=mean(data_gen1), var=var(data_gen1), quantile(data_gen1, c(.1, .9)))\n\n      mean        var        10%        90% \n  319.9844 29298.4895   110.7407   551.6280",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter01.html#exercise-1.8-practice-with-a-variance-covariance-matrix-for-a-bivariate-distribution.",
    "href": "chapter01.html#exercise-1.8-practice-with-a-variance-covariance-matrix-for-a-bivariate-distribution.",
    "title": "Exercises chapter 1",
    "section": "Exercise 1.8 Practice with a variance-covariance matrix for a bivariate distribution.",
    "text": "Exercise 1.8 Practice with a variance-covariance matrix for a bivariate distribution.\nSuppose that you have a bivariate distribution where one of the two random variables comes from a normal distribution with mean \\(μ_X=600\\) and standard deviation \\(σ_X=100\\), and the other from a normal distribution with mean \\(μ_Y=400\\) and standard deviation \\(σ_Y=50\\). The correlation \\(ρ_{XY}\\) between the two random variables is 0.4. Write down the variance-covariance matrix of this bivariate distribution as a matrix (with numerical values, not mathematical symbols), and then use it to generate 100 pairs of simulated data points. Plot the simulated data such that the relationship between the random variables \\(X\\) and \\(Y\\) is clear. Generate two sets of new data (\\(100\\) pairs of data points each) with correlation \\(-0.4\\) and \\(0\\), and plot these alongside the plot for the data with correlation \\(0.4\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exercises chapter 1</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.1-deriving-bayes-rule",
    "href": "chapter02.html#exercise-2.1-deriving-bayes-rule",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.1 Deriving Bayes’ rule",
    "text": "Exercise 2.1 Deriving Bayes’ rule\nLet \\(A\\) and \\(B\\) be two observable events. \\(P(A)\\) is the probability that \\(A\\) occurs, and \\(P(B)\\) is the probability that\n\\(B\\) occurs. \\(P(A|B)\\) is the conditional probability that \\(A\\) occurs given that \\(B\\) has happened. \\(P(A,B)\\) is the joint probability of \\(A\\) and \\(B\\) both occurring.\nYou are given the definition of conditional probability:\n\\[\n\\begin{equation}\nP(A|B)= \\frac{P(A,B)}{P(B)} \\hbox{ where } P(B)&gt;0\n\\end{equation}\n\\]\nUsing the above definition, and using the fact that \\(P(A,B)=P(B,A)\\) (i.e., the probability of\n\\(A\\) and \\(B\\) both occurring is the same as the probability of \\(B\\) and \\(A\\) both occurring), derive an expression for\n\\(P(B|A)\\). Show the steps clearly in the derivation.\n\n\\[\n\\begin{aligned}\nP(B|A) &= \\frac{P(B,A)}{P(A)} \\hbox{ where } P(A)&gt;0 \\\\\n       &= \\frac{P(A|B)P(B)}{P(A)}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.2-conjugate-forms-1",
    "href": "chapter02.html#exercise-2.2-conjugate-forms-1",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.2 Conjugate forms 1",
    "text": "Exercise 2.2 Conjugate forms 1\n\nComputing the general form of a PDF for a posterior\nSuppose you are given data \\(k\\) consisting of the number of successes, coming from a \\(\\text{Binomial}(n,θ)\\) distribution. Given \\(k\\) successes in \\(n\\) trials coming from a binomial distribution, we define a \\(\\text{Beta}(a,b)\\) prior on the parameter \\(θ\\). Write down the Beta distribution that represents the posterior, in terms of \\(a\\), \\(b\\), \\(n\\), and \\(k\\).\n\n\\[\n\\begin{aligned}\np(\\theta | n,k) &\\propto p(k |n,\\theta) p(\\theta) \\\\\n\\text{Beta}(a+k, b+n-k) &= \\text{Binomial}(k|n,θ)\\times\\text{Beta}(\\theta|a,b)\n\\end{aligned}\n\\]\n\n\n\nPractical application\nWe ask 10 yes/no questions from a subject, and the subject returns 0 correct answers. We assume a binomial likelihood function for these data. Also assume a \\(Beta(1,1)\\) prior on the parameter \\(θ\\), which represents the probability of success. Use the result you derived above to write down the posterior distribution of the \\(θ\\) parameter.\n\n\\[\n\\text{Beta}(1+0, 1+10-0) = \\text{Beta}(1, 11)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.3-conjugate-forms-2",
    "href": "chapter02.html#exercise-2.3-conjugate-forms-2",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.3 Conjugate forms 2",
    "text": "Exercise 2.3 Conjugate forms 2\nSuppose that we perform \\(n\\) independent trials until we get a success (e.g., a heads in a coin toss). For coin tosses, the possible outcomes could be, H, T. The probability of success in each trial is \\(θ\\). Then, the Geometric random variable, call it \\(X\\), gives us the probability of getting a success in\n\\(n\\) trials as follows:\n\\[\n\\begin{equation}\nProb(X=n)=\\theta(1-\\theta)^{ n-1}\\text{, where } n =1,2,...\n\\end{equation}\n\\] Let the prior on \\(θ\\) be \\(Beta(a,b)\\), a beta distribution with parameters \\(a\\),\\(b\\). The posterior distribution is a beta distribution with parameters \\(a^*\\) and \\(b^*\\). Determine these parameters in terms of \\(a\\), \\(b\\), and \\(n\\).\n\n\\[\n\\begin{aligned}\np(\\theta | n) &\\propto p(n |\\theta) p(\\theta) \\\\\np(\\theta | n) &\\propto \\theta(1-\\theta)^{ n-1}\\theta^{a-1}(1-\\theta)^{b-1} \\propto \\theta^a(1-\\theta)^{b+n-2} \\\\\np(\\theta | n) &= Beta(a^*,b^*) = Beta(a+1,b+n-1)\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.4-conjugate-forms-3",
    "href": "chapter02.html#exercise-2.4-conjugate-forms-3",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.4 Conjugate forms 3",
    "text": "Exercise 2.4 Conjugate forms 3\nThe Gamma distribution is defined in terms of the parameters a, b: Ga(a,b). If there is a random variable \\(Y\\) (where \\(y \\gt 0\\)) that has a Gamma distribution as a PDF (\\(Y\\sim \\text{Gamma}(a,b)\\)), then:\n\\[\n\\begin{equation}\nGa(y | a,b)=\\frac{b^a y^{a-1} \\exp\\{-by\\}}{\\Gamma(a)}\n\\end{equation}\n\\]\nSuppose that we have \\(n\\) data points, \\(x_1,…,x_n\\), that are drawn from an exponentially distributed. The exponential likelihood function is:\n\\[\n\\begin{equation}\np(x_1,\\dots,x_n | \\lambda)=\\lambda^n \\exp \\{-\\lambda \\sum_{i=1}^n x_i \\}\n\\end{equation}\n\\]\nIt turns out that if we assume a Ga(a,b) prior distribution for \\(λ\\) and the above Exponential likelihood, the posterior distribution of \\(λ\\) is a Gamma distribution. In other words, the Gamma(a,b) prior on the \\(λ\\) parameter in the Exponential distribution will be written:\n\\[\n\\begin{equation}\nGa(\\lambda | a,b)=\\frac{b^a \\lambda^{a-1} \\exp\\{-b\\lambda\\}}{\\Gamma(a)}\n\\end{equation}\n\\]\nFind the parameters \\(a^′\\) and \\(b^′\\) of the posterior distribution.\n\n\\[\n\\begin{aligned}\np(\\lambda | X) &\\propto p(X |\\lambda) p(\\lambda) \\\\\np(\\lambda | X) &\\propto \\lambda^n \\exp \\left\\{-\\lambda \\sum_{i=1}^n x_i  \\right\\}\\lambda^{a-1} \\exp\\{-b\\lambda\\} \\propto \\lambda^{a+n-1} \\exp \\left\\{-\\lambda \\left(\\sum_{i=1}^n x_i + b\\right)\\right\\}\\\\\np(\\lambda | X) &= Ga(a^′,b^′) = Ga\\left(a+n,b+\\sum_{i=1}^n x_i \\right)\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.5-conjugate-forms-4",
    "href": "chapter02.html#exercise-2.5-conjugate-forms-4",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.5 Conjugate forms 4",
    "text": "Exercise 2.5 Conjugate forms 4\n\nComputing the posterior\nThis is a contrived example. Suppose we are modeling the number of times that a speaker says the word “I” per day. This could be of interest if we are studying, for example, how self-oriented a speaker is. The number of times\n\\(x\\) that the word is uttered in over a particular time period (here, one day) can be modeled by a Poisson distribution (\\(x=0,1,2,…\\)):\n\\[\n\\begin{equation}\nf(x\\mid \\theta) = \\frac{\\exp(-\\theta) \\theta^x}{x!}\n\\end{equation}\n\\]\nwhere the rate \\(θ\\) is unknown, and the numbers of utterances of the target word on each day are independent given \\(θ\\).\nWe are told that the prior mean of \\(θ\\) is 100 and prior variance for \\(θ\\) is 225. This information is based on the results of previous studies on the topic. We will use the Gamma(a,b) density (see previous question) as a prior for\n\\(θ\\) because this is a conjugate prior to the Poisson distribution.\n\nFirst, visualize the prior, a Gamma density prior for \\(\\theta\\) based on the above information.\n\n[Hint: we know that for a Gamma density with parameters a, b, the mean is \\(\\frac{a}{b}\\) and the variance is \\(\\frac{a}{b^2}\\). Since we are given values for the mean and variance, we can solve for a,b, which gives us the Gamma density.]\n\n\\[\n\\left.\n\\begin{array}{l}\n\\frac{a}{b} = 100 \\\\\n\\frac{a}{b^2} = 225\n\\end{array}\n\\right\\}\n\\Longrightarrow\n\\left.\n\\begin{array}{l}\na= \\frac{100 \\times 100}{225} \\\\\nb= \\frac{100}{225}\n\\end{array}\n\\right\\}\n\\]\n\n\n\n\n\n\n\nNext, derive the posterior distribution of the parameter \\(θ\\) up to proportionality, and write down the posterior distribution in terms of the parameters of a Gamma distribution.\n\n\n\\[\n\\begin{aligned}\np(\\theta | X) &\\propto p(X |\\theta) p(\\theta) \\\\\np(\\theta | X) &\\propto \\prod_{i=1}^{n} \\exp(-\\theta) \\theta^{x_i} \\times \\theta^{a-1} \\exp\\{-b\\theta\\} \\\\\n               &\\propto \\exp(-n\\theta)\\theta^{\\sum_{i=1}^{n}x_i}\\theta^{a-1} \\exp\\{-b\\theta\\}\\\\\n               &\\propto \\exp(-(n+b)\\theta)\\theta^{a-1+\\sum_{i=1}^{n}x_i}\\\\\np(\\theta | X) &= Ga(a^*,b^*) = Ga\\left(a + \\sum_{i=1}^n x_i, b + n \\right),\n\\end{aligned}\n\\]\n\n\n\nPractical application\nSuppose we know that the number of “I” utterances from a particular individual is \\(115,97,79,131\\). Use the result you derived above to obtain the posterior distribution. In other words, write down the parameters of the Gamma distribution (call them \\(a^∗\\),\\(b^∗\\)) representing the posterior distribution of \\(θ\\) .\n\n\n\n\n\nNow suppose you get one new data point: 200. Using the posterior \\(Gamma(a^∗,b^∗)\\) as your prior, write down the updated posterior (in terms of the updated parameters of the Gamma distribution) given this new data point. Add the updated posterior to the plot you made above.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter02.html#exercise-2.6-the-posterior-mean-is-a-weighted-mean-of-the-prior-mean-and-the-mle-poisson-gamma-conjugate-case",
    "href": "chapter02.html#exercise-2.6-the-posterior-mean-is-a-weighted-mean-of-the-prior-mean-and-the-mle-poisson-gamma-conjugate-case",
    "title": "Exercises chapter 2",
    "section": "Exercise 2.6 The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)",
    "text": "Exercise 2.6 The posterior mean is a weighted mean of the prior mean and the MLE (Poisson-Gamma conjugate case)\nThe number of times an event happens per unit time can be modeled using a Poisson distribution, whose PMF is:\n\\[\n\\begin{equation}\nf(x\\mid \\theta) = \\frac{\\exp(-\\theta) \\theta^x}{x!}\n\\end{equation}\n\\]\nSuppose that we define a Gamma(a,b) prior for the rate parameter \\(θ\\). It is a fact (see exercises above) that the posterior of the \\(θ\\) parameter is a \\(Gamma(a^∗,b^∗)\\) distribution, where \\(a^∗\\) and \\(b^∗\\) are the updated parameters given the data: \\(θ∼Gamma(a^∗,b^∗)\\).\n\nProve that the posterior mean is a weighted mean of the prior mean and the maximum likelihood estimate (mean) of the Poisson-distributed data, \\(\\bar{x} = \\sum_{i=1}^n x/n\\). Hint: the mean of a Gamma distribution is \\(\\frac{a}{b}\\).\n\nSpecifically, what you have to prove is that:\n\\[\n\\frac{a^*}{b^*} = \\frac{a}{b} \\times \\frac{w_1}{w_1 + w_2} + \\bar{x} \\times \\frac{w_2}{w_1 + w_2}\n\\tag{2.1}\\]\nwhere \\(w_1=1\\) and \\(w_2=\\frac{n}{b}\\)\n\n\\[\n\\begin{aligned}\np(\\theta | X) &= Ga(a^*,b^*) = Ga\\left(a + \\sum_{i=1}^n x_i, b + n \\right) \\\\\n\\mathbb{E}[\\theta|X] &= \\frac{a^*}{b^*} = \\frac{a + \\sum_{i=1}^n x_i}{b + n}\\\\\n& = \\frac{\\frac{a}{b}+\\bar{x}\\frac{n}{b}}{1+\\frac{n}{b}} = \\\\\n&= \\frac{a}{b}\\frac{1}{1+\\frac{n}{b}}+\\bar{x}\\frac{\\frac{n}{b}}{1+\\frac{n}{b}}\n\\end{aligned}\n\\]\n\n\nGiven equation Equation 2.1, show that as \\(n\\) increases (as sample size goes up), the maximum likelihood estimate \\(\\bar{x}\\) dominates in determining the posterior mean, and when \\(n\\) gets smaller and smaller, the prior mean dominates in determining the posterior mean.\n\n\n\\[\n\\begin{aligned}\n\\lim_{n \\to \\infty}\\mathbb{E}[\\theta|X] &= \\bar{x}\\\\\n\\lim_{n \\to 0}\\mathbb{E}[\\theta|X] &= \\frac{a}{b}\\\\\n\\end{aligned}\n\\]\n\n\nFinally, given that the variance of a Gamma distribution is \\(\\frac{a}{b^2}\\), show that as \\(n\\) increases, the posterior variance will get smaller and smaller (the uncertainty on the posterior will go down).\n\n\n\\[\n\\begin{aligned}\n\\mathbb{V}[\\theta|X] &= \\frac{a*}{b*^2} = \\frac{a + \\sum_{i=1}^n x_i}{(b + n)^2} = \\frac{a + n\\bar{x}}{(b + n)^2}\\\\\n\\lim_{n \\to \\infty}\\mathbb{V}[\\theta|X] &= 0\\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exercises chapter 2</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.1-check-for-parameter-recovery-in-a-linear-model-using-simulated-data.",
    "href": "chapter03.html#exercise-3.1-check-for-parameter-recovery-in-a-linear-model-using-simulated-data.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.1 Check for parameter recovery in a linear model using simulated data.",
    "text": "Exercise 3.1 Check for parameter recovery in a linear model using simulated data.\nGenerate some simulated independent and identically distributed data with \\(n=100\\) data points as follows:\nNext, fit a simple linear model with a normal likelihood:\n\\[\n\\begin{equation}\ny_n  \\sim \\mathit{Normal}(\\mu,\\sigma) \\tag{3.12}\n\\end{equation}\n\\]\nSpecify the following priors:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mu &\\sim \\mathit{Uniform}(0, 60000) \\\\\n\\sigma &\\sim \\mathit{Uniform}(0, 2000)\n\\end{aligned}\n\\end{equation}\n\\]\nGenerate posterior predictive distributions of the parameters and check that the true values of the parameters \\(μ=500\\), \\(σ=50\\) are recovered by the model. What this means is that you should check whether these true values lie within the range of the posterior distributions of the two parameters. This is a good sanity check for finding out whether a model can in principle recover the true parameter values correctly.\n\n\nfit3.1 &lt;-\n  brm(y ~ 1,\n      data = data.frame(y),\n      family = gaussian(),\n      prior = c(\n        prior(uniform(0, 60000), class = Intercept, lb = 0, ub = 60000),\n        prior(uniform(0, 2000), class = sigma, lb = 0, ub = 2000)\n      ),\n      chains = 4,\n      iter = 2000,\n      warmup = 1000,\n      file_refit = \"on_change\",\n      file = \"fits/fit3.1\",\n      save_model = \"models/model3.1.stan\"\n  )\n\n\n\n[1] \"mean\"\n\n\n    2.5%      50%    97.5% \n499.9399 510.4989 521.1880 \n\n\n\n\n[1] \"sigma\"\n\n\n    2.5%      50%    97.5% \n45.29420 52.01143 59.85601",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.2-a-simple-linear-model.",
    "href": "chapter03.html#exercise-3.2-a-simple-linear-model.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.2 A simple linear model.",
    "text": "Exercise 3.2 A simple linear model.\n\nFit the model fit_press with just a few iterations, say 50 iterations (set warmup to the default of 25, and use four chains). Does the model converge?\n\n\n\nfit3.2.a &lt;-\n  brm(t ~ 1,\n      data = df_spacebar,\n      family = gaussian(),\n      prior = c(\n        prior(uniform(0, 60000), class = Intercept, lb = 0, ub = 60000),\n        prior(uniform(0, 2000), class = sigma, lb = 0, ub = 2000)\n      ),\n      chains = 4,\n      iter = 50,\n      warmup = 25,\n      file_refit = \"on_change\",\n      file = \"fits/fit3.2.a\",\n      save_model = \"models/model3.2.a.stan\"\n  )\n\nmcmc_plot(fit3.2.a, type=\"trace\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5%\n50%\n97.5%\n\n\n\n\nIntercept\n6.12\n168.04\n444.58\n\n\nsigma\n22.17\n34.09\n783.30\n\n\n\n\n\n\n\n\n\nUsing normal distributions, choose priors that better represent your assumptions/beliefs about finger tapping times. To think about a reasonable set of priors for \\(μ\\) and \\(σ\\), you should come up with your own subjective assessment about what you think a reasonable range of values can be for \\(μ\\) and how much variability might happen. There is no correct answer here, we’ll discuss priors in depth in chapter 6. Fit this model to the data. Do the posterior distributions change?\n\n\n\nfit3.2.b &lt;-\n  brm(t ~ 1,\n      data = df_spacebar,\n      family = gaussian(),\n      prior = c(\n        prior(normal(400, 100), class = Intercept),\n        prior(normal(50, 20), class = sigma)\n      ),\n      chains = 4,\n      iter = 2000,\n      warmup = 1000,\n      file_refit = \"on_change\",\n      file = \"fits/fit3.2.b\",\n      save_model = \"models/model3.2.b.stan\"\n  )\n\nmcmc_plot(fit3.2.b, type=\"trace\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5%\n50%\n97.5%\n\n\n\n\nIntercept\n166.05\n168.77\n171.21\n\n\nsigma\n23.37\n25.04\n26.99",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.3-revisiting-the-button-pressing-example-with-different-priors.",
    "href": "chapter03.html#exercise-3.3-revisiting-the-button-pressing-example-with-different-priors.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.3 Revisiting the button-pressing example with different priors.",
    "text": "Exercise 3.3 Revisiting the button-pressing example with different priors.\n\nCan you come up with very informative priors that influence the posterior in a noticeable way (use normal distributions for priors, not uniform priors)? Again, there are no correct answers here; you may have to try several different priors before you can noticeably influence the posterior.\n\n\n\nfit3.3.a &lt;-\n  brm(t ~ 1,\n      data = df_spacebar,\n      family = gaussian(),\n      prior = c(\n        prior(normal(10000, 10), class = Intercept),\n        prior(normal(10, 1), class = sigma)\n      ),\n      chains = 4,\n      iter = 2000,\n      warmup = 1000,\n      file_refit = \"on_change\",\n      file = \"fits/fit3.3.a\",\n      save_model = \"models/model3.3.a.stan\"\n  )\n\n\n\n\n\n\n\n\n\n\n2.5%\n50%\n97.5%\n\n\n\n\nIntercept\n8,103.79\n8,122.28\n8,139.88\n\n\nsigma\n389.96\n391.03\n392.11\n\n\n\n\n\n\n\n\n\nGenerate and plot prior predictive distributions based on this prior and plot them.\n\n\n\n\n\n\n\nGenerate posterior predictive distributions based on this prior and plot them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.4-posterior-predictive-checks-with-a-log-normal-model.",
    "href": "chapter03.html#exercise-3.4-posterior-predictive-checks-with-a-log-normal-model.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.4 Posterior predictive checks with a log-normal model.",
    "text": "Exercise 3.4 Posterior predictive checks with a log-normal model.\n\nFor the log-normal model fit_press_ln, change the prior of \\(σ\\) so that it is a log-normal distribution with location (\\(μ\\)) of \\(−2\\) and scale (\\(σ\\)) of \\(0.5\\). What does such a prior imply about your belief regarding button-pressing times in milliseconds? Is it a good prior? Generate and plot prior predictive distributions. Do the new estimates change compared to earlier models when you fit the model?\n\n\n\n\n\n\n\n\n\n\n\n\nFor the log-normal model, what is the mean (rather than median) time that takes to press the space bar, what is the standard deviation of the finger tapping times in milliseconds?\n\n\n\nmean [ms]   sd [ms] \n    170.1      23.0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  },
  {
    "objectID": "chapter03.html#exercise-3.5-a-skew-normal-distribution.",
    "href": "chapter03.html#exercise-3.5-a-skew-normal-distribution.",
    "title": "Exercises chapter 3",
    "section": "Exercise 3.5 A skew normal distribution.",
    "text": "Exercise 3.5 A skew normal distribution.\nWould it make sense to use a “skew normal distribution” instead of the log-normal? The skew normal distribution has three parameters: location \\(ξ\\) (this is the lower-case version of the Greek letter \\(Ξ\\), pronounced “chi”, with the “ch” pronounced like the “ch” in “Bach”), scale \\(ω\\) (omega), and shape \\(α\\). The distribution is right skewed if \\(α &gt; 0\\), is left skewed if \\(α&lt;0\\), and is identical to the regular normal distribution if \\(α=0\\). For fitting this in brms, one needs to change family and set it to skew_normal(), and add a prior of class = alpha (location remains class = Intercept and scale, class = sigma).\n\nFit this model with a prior that assigns approximately 95% of the prior probability of alpha to be between 0 and 10.\n\n\\[\n\\left.\n\\begin{array}{r}\n\\alpha \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\nPr(0 \\le \\alpha \\le 10) = .95 \\\\\nPr \\left(\\frac{0-\\mu}{\\sigma} \\le \\mathcal{N}(0,1) \\le \\frac{10-\\mu}{\\sigma} \\right) = .95\n\\end{array}\n\\right\\}\n\\Longrightarrow\n\\left.\n\\begin{array}{l}\n\\frac{0-\\mu}{\\sigma} = -1.96 \\\\\n\\frac{10-\\mu}{\\sigma} = 1.96\n\\end{array}\n\\right\\}\n\\Longrightarrow\n\\left.\n\\begin{array}{l}\n\\mu = 5\\\\\n\\sigma = 5/1.96\n\\end{array}\n\\right\\}\n\\]\n\nGenerate posterior predictive distributions and compare the posterior distribution of summary statistics of the skew normal with the normal and log-normal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstat\nintercept\nsigma\n\n\nnormal\nlog normal\nskew normal\nnormal\nlog normal\nskew normal\n\n\n\n\nmean\n168.71\n170.10\n169.67\n25.07\n23.00\n24.04\n\n\n2.5%\n166.05\n167.76\n167.24\n23.37\n21.32\n22.20\n\n\n97.5%\n171.21\n172.50\n172.24\n26.99\n24.80\n26.03",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exercises chapter 3</span>"
    ]
  }
]